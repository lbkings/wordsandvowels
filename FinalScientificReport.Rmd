---
title             : "Context Effects on Auditory Category Knowledge"
shorttitle        : "Context Effects on Auditory Knowledge"

author: 
  - name          : "Shannon Heald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S University Ave, Chicago, IL 60637"
    email         : "smbowdre@uchicago.edu, laurenbacall@uchicago.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Stephen Van Hedger"
    affiliation   : "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Lauren Kingsly"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "University of Chicago"
  - id            : "2"
    institution   : "University of Western Ontario"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "auditory perception, speech perception, lack of invariance"
wordcount         : "X"

bibliography      : "referencez.bib"

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
---

introduction:

In order for us to perceive the world around us, we must use categorical knowledge. From breaking apart a song, to phonetics used within speech, auditory category knowledge is essential for individuals to understand auditory events [@posner:1968, pg 4]. 
Two areas of category knowledge can be applied to auditory events and are necessary in order for us to infer meaning. The first stems from an associative connection to an acoustic event (e.g. doorbell ring) - an isolated concept [@goldstone:1996]. In this way, similar acoustic patterns may be compared to those that we have within our minds. The second broad area of conceptual objects are referred to as interrelated concepts [@deSaussure:19161959;@goldstone:1996]. When applied to auditory objects, it refers to how auditory objects are mapped to sound patterns which are linked within a web. The interrelatedness of the concepts are then defined by themselves but also through their relation to other concepts [@heald:2014].  Concepts themselves within the web of related concepts provides the avenue for generalization. Therefore, the main difference between isolated and interrelated concepts is that isolated concepts do not have systematicity that interrelated concepts have, and therefore cannot be generalized. The current study draws on the research of our previous study which examined how generalized not knowledge can guide an estimate of a starting tone to match a target tone with experts (those who have absolute pitch) and novices (@heald:2014).  In this study, individuals, regardless of experience, were asked to adjust a probe tone to match the target tone pitch. However, the tone was presented as a continuum or within a word. Instead of looking at the variation between experts and novices, we were interested in looking at how context effects change the bias patterns. The idea would be that for words, individuals would show more bias because the word context encourages a person to hear the sound more categorically. We expect individuals to show more error in reproducing a given token from a word continuum and for vowels there should be less error. This belief is strengthened by findings in the Heald et al. paper as word context enhances the use of category knowledge and as such, it may diminish the ability to replicate the exact token from the continuum heard. We hope to explore this belief by examining the individual deviation between the probe and target tone, finding the error of what people reproduce based on where they heard a vowel continuum or word, and be able to understand what information people retain based on context. 


```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(lattice)
library(magrittr)
r_refs("referencez.bib")
library(knitr)
library(ggpubr)

synthvowelslong <- read.csv("SynthVowels.csv")
synthwordslong <-read.csv("SynthWords.csv")

colnames(synthvowelslong)
synthvowelslong %>% select(Gesture:C14T) 
synthvowelsnew <- synthvowelslong %>%select(Gesture:C14T) 
colnames(synthwordslong)
synthwordslong %>% select(Gesture:C14T)
synthwordsnew <- synthwordslong %>% select(Gesture:C14T)
synthwordsnew %>% select(-c(High.Low.Probe))
synthwordsnew <-synthwordsnew %>% select(-c(High.Low.Probe))
synthvowels <- synthvowelsnew %>% 
  add_column(embedded = "False", .after = "C14T")
synthwords <- synthwordsnew %>%
  add_column(embedded = "True")
wordsandvowels <- rbind(synthvowels, synthwords)
contexteffects <- wordsandvowels %>% 
  mutate(Probe = case_when(PTPosition > 9 ~ "High", 
                             PTPosition <= 9 ~ "Low")) %>% 
  group_by(Subject, embedded, TTPosition, Probe) %>% 
  summarise(MeanPosDiff = mean(PositionDiff)) 
editedcontext <- read.csv("ContextKnowledgeVowelWords.csv")

dataset <- "context effects"
datasetz <- "wordsandvowels"
```


# Methods
We based our methodology from our previous paper that examined auditory category knowledge within experts and novices.[^1] <!-- Auditory category knowledge in experts and novices (Heald, Van Hedger & Nusbaum, 2014; retrieved from https://www.frontiersin.org/articles/10.3389/fnins.2014.00260/full) -->

Nineteen undergraduates were recruited from the University of Chicago community and were between 18 and 26 years of age. It was not a requirement of the study for individuals to have musical experience, however, we asked participants to report their music experience in a questionnaire after the study. Participants were either given course credit or paid for their participation in the study. All participants did not report a history of a speech or hearing disorder. 

The experiment itself consisted of a tone adjustment task where a target tone was matched by varying the frequency of a probe tone (sinewave tones were generated using Matlab). On each trial, participants were asked to click the button with a T on it to hear the target tone, followed by one second of white noise. Individuals had to then press a button labeled P to hear the probe tone. The timing between listening to the target tone and the probe tone was not predetermined but rather set by the participant. Individuals were then asked to try their best to adjust the probe tone to match the target tone they first heard. Participants adjusted the probe tone by manually clicking and listening to other tones that were present on the screen after the presentation of the target tone. The participants could arrange the tones in any fashion they wanted, with the goal of placing the tone that sounded most like the target tone in the first position (or top of the screen) and the least likely option last. These tones were comprised of tones that were higher in frequency in the series as well as tones that were lower in frequency. Higher frequency tones modified the target tone in 66-cent increments, while the lower frequency tones modified the target tone in 33 cent increments. A step size of 33 cents was chosen because it is above most listeners threshold for noticing pitch differences. Participants were given as long as they needed to adjust the probe tone. When participants were certain of their answer, they confirmed their answer and continued onto the next trial. This was a within subjects design. Within the vowel condition, participants heard a continuum of vowels that varied in acoustic frequency range in both the target and probe tones. In the second condition, participants heard a vowel and then heard a word that had the same or similar vowel as the target vowel. There were 18 probes in total across both conditions, and there were multiple target tones used across trials that were counterbalanced in a randomized fashion to omit any over or underestimation of the tones. 


[^1]: Heald, S. L., Van Hedger, S. C., & Nusbaum, H. C. (2014). Auditory category knowledge in experts and novices. Frontiers in Neuroscience, 8, 260.

# Results

While results are preliminary, the first calculation was examining if there was a difference in error bias (represents how many 33-cent steps an individual took towards reaching what they perceived was correct) based on whether the tone presented to the individual started high or low. To visualize this, we first combined both datasets (words and vowels) and indicated whether the row corresponded to a word (embedded) or a vowel (not embedded) that the participant heard. We used two datasets, `r dataset` and `r datasetz` which were tidied and cleaned in R prior to data analysis.The tones were delineated based on whether they were high pitch or low pitch with 1-9 (exclusive) representing low tones, and 9-18 (inclusive) representing high tones. Our first visual comparison was to see whether the Probe had an impact on the error bias the participant demonstrated. Mean Position Difference was calculated by taking the position of the actual target tone and subtracting it from the position of the tone the individual thought was correct. As referenced in Table\ \@ref(tab:table-MeanPosDiff) each subject bias toward the tones, or mean position difference between the probe and target tone is displayed. Overall, half of the individuals overestimated the target tone, while the other half underestimated the target tone. Figure\ \@ref(fig:fig-tonescale), however, represents the range of tones used in the experiment and the starting tones indicate the probe tones. This figure is based on the [-@heald:2014] paper that this experiment is predicated on. 
In Figure\ \@ref(fig:GGPlotBoxplot) we can see that the there was a difference in bias between participants identification of vowels versus words when given a high tone compared to a lower tone. Specifically, individuals tend to overestimate the target tone position when presented with a high probe first compared to a lower tone probe. Individuals are more likely to underestimate the position of the target tone when presented with a low tone probe, however, the effect is most prominent for words rather than vowels. This finding would be in line with our hypothesis that individuals would hear the sound more categorically within in a word, and thus showcase more bias because of the context effect. Likewise, individuals that heard the higher probe tone showed more overestimation for the embedded condition (words) than vowels. While the boxplot shows some evidence for differing error bias rates based on the probe tone, we wanted to further investigate whether there was a crossover point whereby a low tone effects the mean position difference of a vowel more . To examine this, we created a line plot seen in Figure\ \@ref(fig:GGPlotLine). Visual examination yields that can see in Figure\ \@ref(fig:GGPlotLine) that a word or vowel (embedded versus non-embedded) effects the overall bias for the tones in a different way. This graph also indicates that high probe tones presented with vowels embedded within a word (embedded) have a higher bias than vowels themselves. Overall, this graph depicts that words have a higher bias rate when presented with both a high or low tone probe comparatively speaking to a vowel. The results from visual inspection also fall within our hypothesis that words will elicit more bias as it enhances category knowledge and thus reduces the likelihood that the individual can recreate the actual sound heard. To further disambiguate the relationship between the target tone position and bias a participant had toward estimating the correct tone, we conducted a 2-way ANOVA comparing the effect of “TTPosition” on “MeanPosDiff”, two variables in our dataset that represent target tone position and bias, respectively. This analysis yielded a very strong p-value of a < 0.01. From this analysis, we can infer that there is an effect of the target tone position (low or high) on the overall bias a participant demonstrates on a given trial. Further examination of the data will include looking at individual crossover points within each of the 19 subjects to see when the bias between words and vowels flips based on their starting tone, and how bias changes over the duration of a trial. 


# Discussion
Taken together, the preliminary analyzed data from this experiment provide evidence that context effects can enhance categorical knowledge, and thus provides supporting evidence for lack of invariance problem. Here, listeners had more difficulty with the acoustic variability of the tone for words because of the heightened use of categorical knowledge whereas that context effect was not present for the vowel continuum. Thus, we have showcased empirical evidence that context effects modulate the accuracy of individuals selecting tones.  



(ref:GGPlotBoxplot) Boxplot of Speech Perception Differences Between Words and Vowels

``` {r, GGPlotBoxplot, fig.cap = "(ref:GGPlotBoxplot)"}
#graphs for my dataset 
ggplot(contexteffects, aes(Probe, MeanPosDiff, color = embedded)) + geom_boxplot() + xlab("Probe") + ylab("Mean Position Difference") 

```


(ref:GGPlotLine) Difference in Context Effects for Words and Vowels

```{r, GGPlotLine, fig.cap = "(ref:GGPlotLine)"}
ggline(contexteffects, "Probe", "MeanPosDiff", 
       linetype = "embedded", color = "embedded", add = "mean_se")
```

(ref:fig-tonescale-caption) Diagram of the range of tones used for the probe and target tones

```{r fig-tonescale, echo = FALSE, fig.cap = "(ref:fig-tonescale-caption)"}
#figure of tones 
knitr::include_graphics("tones.png")

```



```{r table-MeanPosDiff}
mytable <- contexteffects %>%
  group_by(Subject) %>%
  summarize(
    mean.MeanPosDiff = mean(MeanPosDiff) 
  ) %>% apa_table(caption = "Overall Bias in Correct Tone Position Selection for Subjects")
mytable
```












(ref:fig-anova-ttpositionmeanposdiff-caption) Anova analysis yields a significant effect of Target Tone Position effecting the Mean Position Difference or bias an individual has towards selecting the correct tone. 


```{r, anova-ttpositionmeanposdiff}

model <- aov(TTPosition ~ MeanPosDiff, data = contexteffects)
summary(model)

```



# References
::: {#refs custom-style="Bibliography"}
:::


