---
title             : "Context Effects on Auditory Category Knowledge"
shorttitle        : "Context Effects on Auditory Knowledge"

author: 
  - name          : "Shannon Heald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S University Ave, Chicago, IL 60637"
    email         : "smbowdre@uchicago.edu, laurenbacall@uchicago.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Stephen Van Hedger"
    affiliation   : "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Lauren Kingsly"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "University of Chicago"
  - id            : "2"
    institution   : "University of Western Ontario"

authornote: |
 Original experimental procedure took place in 2012
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "auditory perception, speech perception, lack of invariance"
wordcount         : "X"

bibliography      : "referencez.bib"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
---

Introduction: <!-- ND: this should be a header -->

In order for us to perceive the world around us, we must use categorical knowledge. From breaking apart a song, to phonetics used within speech, auditory category knowledge is essential for individuals to understand auditory events [@posner:1968, pg 4]. 
Two areas of category knowledge can be applied to auditory events and are necessary in order for us to infer meaning. The first stems from an associative connection to an acoustic event (e.g. doorbell ring) - an isolated concept [@robertgoldstone1996]. In this way, similar acoustic patterns may be compared to those that we have within our minds. The second broad area of conceptual objects are referred to as interrelated concepts [@deSaussure:19161959; @robertgoldstone1996]. When applied to auditory objects, it refers to how auditory objects are mapped to sound patterns which are linked within a web. The interrelatedness of the concepts are then defined by themselves but also through their relation to other concepts [@heald:2014]. Concepts themselves within the web of related concepts provides the avenue for generalization. Therefore, the main difference between isolated and interrelated concepts is that isolated concepts do not have systematicity that interrelated concepts have, and therefore cannot be generalized. The current study draws on the research of our previous study which examined how generalized note knowledge can guide an estimate of a starting tone to match a target tone with experts (those who have absolute pitch) and novices (@heald:2014). In this study, individuals, regardless of experience, were asked to adjust a probe tone to match the target tone pitch. However, the tone was presented as a continuum or within a word. Instead of looking at the variation between experts and novices, we were interested in looking at how context effects change the bias patterns (@heald:2014). The idea would be that for words, individuals would show more bias because the word context encourages a person to hear the sound more categorically. We expect individuals to show more error in reproducing a given token from a word continuum and for vowels there should be less error. This belief is strengthened by findings in the Heald et al. paper as word context enhances the use of category knowledge and as such, it may diminish the ability to replicate the exact token from the continuum heard. We hope to explore this belief by examining the individual deviation between the probe and target tone, finding the error of what people reproduce based on whether they heard a vowel continuum or word, and be able to understand what information people retain based on context. 


```{r setup, include = FALSE}

## ND: Not strictly necessary, but typically this setup chunk would go immediately after the yaml header and before any 

library("papaja")
library(tidyverse)
library(lattice)
library(magrittr)
r_refs("referencez.bib")
library(knitr)
library(ggpubr)

synthvowelslong <- read.csv("SynthVowels.csv")
synthwordslong <-read.csv("SynthWords.csv")

colnames(synthvowelslong)
synthvowelslong %>% select(Gesture:C14T) 
synthvowelsnew <- synthvowelslong %>%select(Gesture:C14T) 
colnames(synthwordslong)
synthwordslong %>% select(Gesture:C14T)
synthwordsnew <- synthwordslong %>% select(Gesture:C14T)
synthwordsnew %>% select(-c(High.Low.Probe))
synthwordsnew <-synthwordsnew %>% select(-c(High.Low.Probe))
synthvowels <- synthvowelsnew %>% 
  add_column(embedded = "False", .after = "C14T")
synthwords <- synthwordsnew %>%
  add_column(embedded = "True")
wordsandvowels <- rbind(synthvowels, synthwords)
contexteffects <- wordsandvowels %>% 
  mutate(Probe = case_when(PTPosition > 9 ~ "High", 
                             PTPosition <= 9 ~ "Low")) %>% 
  group_by(Subject, embedded, TTPosition, Probe) %>% 
  summarise(MeanPosDiff = mean(PositionDiff)) 
editedcontext <- read.csv("ContextKnowledgeVowelWords.csv")

dataset <- "context effects"
datasetz <- "wordsandvowels" ## ND: datasetz & referencez love it
```


# Methods
We based our methodology from our previous paper that examined auditory category knowledge within experts and novices.[^1] <!-- Auditory category knowledge in experts and novices (Heald, Van Hedger & Nusbaum, 2014; retrieved from https://www.frontiersin.org/articles/10.3389/fnins.2014.00260/full) -->

Twenty undergraduates were recruited from the University of Chicago community and were between 18 and 26 years of age. It was not a requirement of the study for individuals to have musical experience, however, we asked participants to report their music experience in a questionnaire after the study. Participants were either given course credit or paid for their participation in the study. All participants did not report a hearing disorder. 

The experiment itself consisted of a tone adjustment task where a target tone was matched by varying the frequency of a probe tone (sinewave tones were generated using Matlab). On each trial, participants were asked to click the button with a T on it to hear the target tone, followed by one second of white noise (@heald:2014). <!-- ND: see my note about bibtex at the end --> Individuals had to then press a button labeled P to hear the probe tone. The timing between listening to the target tone and the probe tone was not predetermined but rather set by the participant. Individuals were then asked to try their best to adjust the probe tone to match the target tone they first heard. Participants adjusted the probe tone by manually clicking and listening to other tones that were present on the screen after the presentation of the target tone. The participants could arrange the tones in any fashion they wanted, with the goal of placing the tone that sounded most like the target tone in the first position (or top of the screen) and the least likely option last. These tones were comprised of tones that were higher in frequency in the series as well as tones that were lower in frequency. Higher frequency tones modified the target tone in 66-cent increments, while the lower frequency tones modified the target tone in 33 cent increments. A step size of 33 cents was chosen because it is above most listeners threshold for noticing pitch differences. Participants were given as long as they needed to adjust the probe tone. When participants were certain of their answer, they confirmed their answer and continued onto the next trial. This was a within subjects design. Within the vowel condition, participants heard a continuum of vowels that varied in acoustic frequency range in both the target and probe tones. In the second condition, participants heard a vowel and then heard a word that had the same or similar vowel as the target vowel. There were 18 probes in total across both conditions, and there were multiple target tones used across trials that were counterbalanced in a randomized fashion to omit any over or underestimation of the tones. 


[^1]: Heald, S. L., Van Hedger, S. C., & Nusbaum, H. C. (2014). Auditory category knowledge in experts and novices. Frontiers in Neuroscience, 8, 260.

# Results

While results are preliminary, the first calculation was examining if there was a difference in error bias (represents how many 33-cent steps an individual took towards reaching what they perceived was correct) based on whether the tone presented to the individual started high or low. To visualize this, we first combined both datasets (words and vowels) and indicated whether the row corresponded to a word (embedded) or a vowel (not embedded) that the participant heard. We used two datasets, `r dataset` and `r datasetz` which were tidied and cleaned in R prior to data analysis.The tones were delineated based on whether they were high pitch or low pitch with 1-9 (exclusive) representing low tones, and 9-18 (inclusive) representing high tones. Our first visual comparison was to see whether the Probe had an impact on the error bias the participant demonstrated. Mean Position Difference was calculated by taking the position of the actual target tone and subtracting it from the position of the tone the individual chose/thought was correct. As referenced in Table\ \@ref(tab:table-MeanPosDiff) each subject bias toward the tones, or mean position difference between the probe and target tone is displayed. Overall, half of the individuals overestimated the target tone, while the other half underestimated the target tone. Figure\ \@ref(fig:fig-tonescale), however, represents the range of tones used in the experiment and the starting tones indicate the probe tones. This figure is based on the [-@heald:2014] paper that this experiment is predicated on. 
In Figure\ \@ref(fig:GGPlotBoxplot) we can see that the there was a difference in bias between participants identification of vowels versus words when given a high tone compared to a lower tone. Specifically, individuals tend to overestimate the target tone position when presented with a high probe first compared to a lower tone probe. Individuals are more likely to underestimate the position of the target tone when presented with a low tone probe, however, the effect is most prominent for words rather than vowels. This finding would be in line with our hypothesis that individuals would hear the sound more categorically within in a word, and thus showcase more bias because of the context effect. Likewise, individuals that heard the higher probe tone showed more overestimation for the embedded condition (words) than vowels. While the boxplot shows some evidence for differing error bias rates based on the probe tone, we wanted to further investigate whether there was a crossover point whereby a low tone effects the mean position difference of a vowel more . To examine this, we created a line plot seen in Figure\ \@ref(fig:GGPlotLine). Visual examination, as seen in Figure\ \@ref(fig:GGPlotLine), showcases that a word or vowel (embedded versus non-embedded) effects the overall bias for the tones in a different way. This graph also indicates that high probe tones presented with vowels embedded within a word (embedded) have a higher bias than vowels themselves. Overall, this graph depicts that words have a higher bias rate when presented with both a high or low tone probe comparatively speaking to a vowel. The results from visual inspection also fall within our hypothesis that words will elicit more bias as it enhances category knowledge and thus reduces the likelihood that the individual can recreate the actual sound heard. To further disambiguate the relationship between the target tone position and bias a participant had toward estimating the correct tone, we conducted a 2-way ANOVA <!-- ND: If I'm interpreting your intentions correctly here I think you're using a 1-way anova for this analysis --> comparing the effect of “TTPosition” on “MeanPosDiff”, two variables in our dataset that represent target tone position and bias, respectively. This analysis yielded a very strong p-value of a < 0.01. From this analysis, we can infer that there is an effect of the target tone position (low or high) on the overall bias a participant demonstrates on a given trial. Further examination of the data will include looking at individual crossover points within each of the 19 subjects to see when the bias between words and vowels flips based on their starting tone, and how bias changes over the duration of a trial. 

<!-- 

ND: You should include the code chunk with your anova here where you talk about it. You've saved your anova as "model". If you also also use summary(model) you can easily retrieve all the info you'd want to include here! So let's say we saved sum.mod <- summary(model) in the anova chunk. Take a look at what that object looks like (very similar to a table, so you can refer to "cells" with [[square brackets]]... then we can say here: 

A 1-way ANOVA demonstrated a significant effect of tone position on bias $F$(`r sum.mod[[1]][["Df"]][[1]]`, `r sum.mod[[1]][["Df"]][[2]]`) = `r sum.mod[[1]][["F value"]][[1]]`, $p$ `r printp(sum.mod[[1]][["Pr(>F)"]][[1]])`.

-->



# Discussion
Taken together, the preliminary analyzed data from this experiment provide evidence that context effects can enhance categorical knowledge, and thus provides supporting evidence for the lack of invariance problem (@appelbaum:1996). Here, listeners had more difficulty with the acoustic variability of the tone for words because of the heightened use of categorical knowledge whereas that context effect was not present for the vowel continuum. Thus, we have showcased empirical evidence that context effects modulate the accuracy of individuals selecting tones.  



(ref:GGPlotBoxplot) Boxplot of speech perception differences between words and vowels

``` {r, GGPlotBoxplot, fig.cap = "(ref:GGPlotBoxplot)"}
#graphs for my dataset 
ggplot(contexteffects, aes(Probe, MeanPosDiff, color = embedded)) + geom_boxplot() + xlab("Probe") + ylab("Mean Position Difference") 

## ND: Cool! I love how these figures turned out. And I love how simple and clean the code is!!!

```


(ref:GGPlotLine) Difference in context effects for words and vowels

```{r, GGPlotLine, fig.cap = "(ref:GGPlotLine)"}
ggline(contexteffects, "Probe", "MeanPosDiff", 
       linetype = "embedded", color = "embedded", add = "mean_se")
```

(ref:fig-tonescale-caption) Diagram of the range of tones used for the probe and target tones

```{r fig-tonescale, echo = FALSE, fig.cap = "(ref:fig-tonescale-caption)"}
#figure of tones 
knitr::include_graphics("tones.png")

```



```{r table-MeanPosDiff}
mytable <- contexteffects %>%
  group_by(Subject) %>%
  summarize(
    mean.MeanPosDiff = mean(MeanPosDiff) 
  ) %>% apa_table(caption = "Overall Bias in Correct Tone Position Selection for Subjects")
mytable

## ND: Try to avoid leaving raw variable names in tables and figures. Use reader-friendly column headers like "Mean position difference" rather than the raw name "mean.MeanPosDiff"
```












(ref:fig-anova-ttpositionmeanposdiff-caption) Anova analysis yields a significant effect of Target Tone Position effecting the Mean Position Difference or bias an individual has towards selecting the correct tone. <!-- ND: The anova is a model, not a figure, so it doesn't need a caption. If you look at your pdf you can see that this chunk doesn't display anything anywhere in the document and this caption doesn't appear anywhere either (which makes sense, because you're running and saving a model and not actually asking it to print/display anything!).-->


```{r, anova-ttpositionmeanposdiff}

model <- aov(TTPosition ~ MeanPosDiff, data = contexteffects)



```



# References
::: {#refs custom-style="Bibliography"}
:::

<!-- 

Great work, Lauren! You've made SO MUCH PROGRESS here! The only piece that's a little lacking is the inline `r code` that we covered on the very last day of class. Check out my suggestions in your results section for some ideas of what this would look like. I do see that you used it to print two strings you set as variables (valid!), but it's so much more versatile than that! I really encourage you to play around with it (in a non-graded, no-pressure situation). It's not a massive part of the assignment, but it can be massively helpful to you going forward if you want to keep using R markdown in your research.

OK SO the bibtex thing you emailed about! I actually didn't look closely enough at your problem in the screenshot you sent me and thought it was something else (sorry!). What's actually going on here is, I believe, a combination of 2 things: 1) including the ":" in your bibtex keys (which I is technically allowed in bibtex breaks some mystery part of the knitting process) and 2) double curly brackets around the author fields. Using {{Some text}} tells knitr to print, literally, "Some text" rather than interpreting that field as the authors and reformatting appropriately. I'm definitely not taking any points off for this and I'm taking full responsibility for giving you bad information earlier!! Just for future troubleshooting. :)

As a general note, it's common practice to not refer to datasets, variables, etc by their literal names in code (e.g., "datasetz", "MeanPosDiff") in the actual narrative text of your report. When you're just writing, you can use plain English, easy to interpret language. Then in the code chunks you can add comments with additional information about what the specific tables/variables/whatever actually are. This way if someone is just reading your paper for the content (less interested in methods or analysis) they can easily understand what you're describing without looking at code or thinking about things in a "programmy" way. But then if someone *does* want to think about your methods/analysis they can easily connect the dots between your writing and your code when they look for it.

It's been a lot of fun watching this project come together. I know you've got a ways to go with this research, but I hope you're proud of the tremendous amount you've already accomplished. This is going to be a really cool paper!

19/20

-->


